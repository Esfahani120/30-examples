{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3156fac3",
   "metadata": {},
   "source": [
    "Here's a simple example of a PyTorch program for the MNIST dataset. This example will use a simple feed-forward neural network.\n",
    "The provided code is a simple implementation of a neural network that learns to classify the MNIST dataset, which consists of 28x28 pixel images of handwritten digits (0-9).\n",
    "\n",
    "# Here's a high-level view of what's happening:\n",
    "\n",
    "#### Data Loading: \n",
    "The code first downloads the MNIST dataset and sets it up in PyTorch DataLoader objects, which will efficiently generate mini-batches of the data during training.\n",
    "\n",
    "#### Model Definition: \n",
    "A simple feed-forward neural network is defined with one hidden layer and a ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "#### Training: \n",
    "The model is then trained over a certain number of epochs (full passes through the training dataset). During each epoch, it uses backpropagation and gradient descent to adjust the model's weights based on its performance.\n",
    "\n",
    "#### Testing: \n",
    "After training, the model's performance is evaluated on a test set (which the model has not seen during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d927f4a",
   "metadata": {},
   "source": [
    "# Now, let's go through the code step by step:\n",
    "\n",
    "#### Import Libraries: \n",
    "All the necessary libraries are imported first. These include torch, torch.nn for the neural network, torchvision for downloading and loading the MNIST dataset, and torchvision.transforms for preprocessing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6880f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f6bb8",
   "metadata": {},
   "source": [
    "#### Define the Device: \n",
    "The code sets the device to be used for computations, either CPU or GPU, depending on what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd9aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d6b38",
   "metadata": {},
   "source": [
    "#### Hyperparameters Definition: \n",
    "Some hyperparameters are set, such as the input size (28x28 pixels for MNIST images, hence 784), the hidden layer size, the number of output classes (10 digits), the number of epochs (complete passes through the dataset), the batch size, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd9e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10 \n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ae482",
   "metadata": {},
   "source": [
    "#### Load the Dataset: \n",
    "The MNIST dataset is downloaded and loaded into memory. The images are transformed into tensors using transforms.ToTensor()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5d6d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.0%5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%C:\\Users\\Sandisk\\anaconda3\\envs\\pytorch171_cu110\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbc434",
   "metadata": {},
   "source": [
    "#### DataLoader Definition: \n",
    "PyTorch's DataLoader is defined for both the training and testing datasets. It allows efficient iteration over the data.\n",
    "\"train_dataset\" provides the interface for accessing your data, and \"train_loader\" provides an iterator to loop over the data in manageable batches, rather than loading the entire dataset into memory at once. This is especially important when working with large datasets that don't fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e37ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68bc9a6",
   "metadata": {},
   "source": [
    "#### Data visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48045164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose an index of an image you want to display\n",
    "image_index = 0 \n",
    "\n",
    "# Access the image and label at the chosen index\n",
    "image, label = train_dataset[image_index] #or we can use: image, label = train_loader.dataset[image_index]\n",
    "\n",
    "# Display the image and its label\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22e913",
   "metadata": {},
   "source": [
    "#### Neural Network Definition: \n",
    "A simple neural network with one hidden layer and a ReLU activation function is defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775d22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__() \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33634833",
   "metadata": {},
   "source": [
    "Line-by-line explanation:\n",
    "\n",
    "##### class NeuralNet(nn.Module): \n",
    "We define a class NeuralNet that inherits from nn.Module. nn.Module is the base class for all neural network modules in PyTorch. Our network is a subclass of it. When you see 'class NeuralNet(nn.Module):', think of it as \"I'm creating a new PyTorch neural network class that has all the standard features of a PyTorch network\".\n",
    "\n",
    "##### def _ _ init _ _(self, input_size, hidden_size, num_classes): \n",
    "This is the initializer of the class. It takes as input the size of the input layer, the size of the hidden layer, and the number of classes (output size).\n",
    "\n",
    "##### super(NeuralNet, self)._ _ init _ _(): \n",
    "This calls the initializer of the parent class (nn.Module). This is necessary for the PyTorch internals to work correctly. In Python 3, you can simply write \"super()._ _ init _ _()\"\n",
    "\n",
    "##### self.fc1 = nn.Linear(input_size, hidden_size): \n",
    "This creates a fully connected layer (fc1) that connects the input to the hidden layer. It has input_size number of inputs and hidden_size number of outputs.\n",
    "\n",
    "##### self.relu = nn.ReLU(): \n",
    "This defines the activation function ReLU (Rectified Linear Unit) which will be applied after the first fully connected layer.\n",
    "\n",
    "##### self.fc2 = nn.Linear(hidden_size, num_classes): \n",
    "This creates another fully connected layer (fc2) that connects the hidden layer to the output layer. It has hidden_size number of inputs and num_classes number of outputs.\n",
    "\n",
    "##### def forward(self, x): \n",
    "This defines the forward pass function. It takes as input a batch of images x.\n",
    "\n",
    "##### out = self.fc1(x), out = self.relu(out), out = self.fc2(out): \n",
    "These lines implement the actual forward pass. The input data is passed through fc1, then the ReLU activation function is applied to the outputs of fc1, and then the result is passed through fc2.\n",
    "\n",
    "##### return out: \n",
    "The output of the network is returned.\n",
    "\n",
    "##### model = NeuralNet(input_size, hidden_size, num_classes).to(device): \n",
    "An instance of the NeuralNet class is created. This line also moves the model to the GPU if one is available. The device object was defined earlier in the code as device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365a4a9",
   "metadata": {},
   "source": [
    "#### Loss Function and Optimizer: \n",
    "The loss function (CrossEntropyLoss in this case, suitable for multi-class classification) and the optimizer (Adam) are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd131037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "# Adam, which is a type of gradient descent method that adapts the learning rate for each weight in the model. \n",
    "# We pass it our model's parameters and the learning rate we defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba759ee2",
   "metadata": {},
   "source": [
    "#### Training Loop: \n",
    "The training loop is where the model learns. For each epoch, and each batch of images and labels, the model does a forward pass (computes the output and loss), a backward pass (computes the gradients), and an optimization step (updates the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48237bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.5041453838348389\n",
      "Epoch [1/5], Step [200/600], Loss: 0.410756379365921\n",
      "Epoch [1/5], Step [300/600], Loss: 0.10506825149059296\n",
      "Epoch [1/5], Step [400/600], Loss: 0.12469432502985\n",
      "Epoch [1/5], Step [500/600], Loss: 0.12765644490718842\n",
      "Epoch [1/5], Step [600/600], Loss: 0.11365668475627899\n",
      "Epoch [2/5], Step [100/600], Loss: 0.23069995641708374\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1140487864613533\n",
      "Epoch [2/5], Step [300/600], Loss: 0.14248181879520416\n",
      "Epoch [2/5], Step [400/600], Loss: 0.08079265803098679\n",
      "Epoch [2/5], Step [500/600], Loss: 0.15917660295963287\n",
      "Epoch [2/5], Step [600/600], Loss: 0.09922943264245987\n",
      "Epoch [3/5], Step [100/600], Loss: 0.11063677817583084\n",
      "Epoch [3/5], Step [200/600], Loss: 0.15619051456451416\n",
      "Epoch [3/5], Step [300/600], Loss: 0.06903975456953049\n",
      "Epoch [3/5], Step [400/600], Loss: 0.07276370376348495\n",
      "Epoch [3/5], Step [500/600], Loss: 0.12367439270019531\n",
      "Epoch [3/5], Step [600/600], Loss: 0.05386575683951378\n",
      "Epoch [4/5], Step [100/600], Loss: 0.08303815126419067\n",
      "Epoch [4/5], Step [200/600], Loss: 0.05996132269501686\n",
      "Epoch [4/5], Step [300/600], Loss: 0.02419722080230713\n",
      "Epoch [4/5], Step [400/600], Loss: 0.04374012351036072\n",
      "Epoch [4/5], Step [500/600], Loss: 0.05054358020424843\n",
      "Epoch [4/5], Step [600/600], Loss: 0.060352109372615814\n",
      "Epoch [5/5], Step [100/600], Loss: 0.02905954048037529\n",
      "Epoch [5/5], Step [200/600], Loss: 0.03876379132270813\n",
      "Epoch [5/5], Step [300/600], Loss: 0.010361245833337307\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0878579393029213\n",
      "Epoch [5/5], Step [500/600], Loss: 0.043398041278123856\n",
      "Epoch [5/5], Step [600/600], Loss: 0.008335295133292675\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c2a02",
   "metadata": {},
   "source": [
    "##### for i, (images, labels) in enumerate(train_loader): \n",
    "This line starts a loop over our training data loader. This loop will yield batches of images and their corresponding labels.\n",
    "\n",
    "##### images = images.reshape(-1, 28*28).to(device), labels = labels.to(device): \n",
    "These lines reshape our images into vectors (since our model is a simple fully connected neural network and can't handle 2D images) and moves the images and labels to the GPU if available.\n",
    "\n",
    "##### outputs = model(images): \n",
    "Here we pass our images to our model, which performs a forward pass through the network and returns the output. When you call an instance of your model like a function (like outputs = model(images) in your code), under the hood, PyTorch actually calls your model's forward method. This is done through the _ _ call _ _ method implemented in the base nn.Module class, which your model inherits from. In this code, when you do \"outputs = model(images)\", it's equivalent to doing \"outputs = model.forward(images)\". But the recommended way is to call the model instance directly because it's not only shorter, but also because the _ _ call _ _ method includes some additional steps before and after the forward pass (like handling nn.Module hooks).\n",
    "\n",
    "##### loss = criterion(outputs, labels): \n",
    "The output of our model and the true labels are passed to our loss function, which calculates the loss - a measure of how far our model's predictions are from the true values.\n",
    "\n",
    "##### optimizer.zero_grad(): \n",
    "This line resets all gradients in the model. This is important because PyTorch accumulates gradients.\n",
    "\n",
    "##### loss.backward(): \n",
    "This line calculates the gradient of the loss with respect to the model parameters. This is done using the backpropagation algorithm.\n",
    "\n",
    "##### optimizer.step(): \n",
    "This line performs a parameter update based on the currently calculated gradients stored in the model parameters.\n",
    "\n",
    "##### if (i+1) % 100 == 0: ...: \n",
    "This line of code is logging our training progress. Every 100 batches, it prints out the current epoch and batch number, and the loss on that batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3378af1",
   "metadata": {},
   "source": [
    "#### Testing: \n",
    "Finally, the model is evaluated on the test dataset. Gradients are not needed for this step, so torch.no_grad() is used to save memory. The percentage of correctly classified images is printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c482e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.92 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da5d15",
   "metadata": {},
   "source": [
    "##### with torch.no_grad(): \n",
    "This context manager tells PyTorch that we do not want to compute or store gradients in the following block of code. This reduces memory usage and speeds up computation, and is useful when we're only doing forward passes through the network, like during inference/testing.\n",
    "\n",
    "##### correct = 0, total = 0: \n",
    "We set up counters for the total number of predictions and the number of correct predictions.\n",
    "\n",
    "##### for images, labels in test_loader: \n",
    "This starts a loop over the test data loader. For each iteration, it gives a batch of images and their corresponding labels.\n",
    "\n",
    "##### images = images.reshape(-1, 28*28).to(device), labels = labels.to(device): \n",
    "These lines reshape our images into vectors and move the images and labels to the GPU if available.\n",
    "\n",
    "##### outputs = model(images): \n",
    "Here we pass our images to our model, which performs a forward pass through the network and returns the output.\n",
    "\n",
    "##### _, predicted = torch.max(outputs.data, 1): \n",
    "This line gets the class with the highest prediction probability from our model's output. The torch.max() function returns both the maximum value and the index of the maximum value along a given dimension (in this case, dimension 1, which is the dimension of the class probabilities). We only want the indices (i.e., the predicted classes), so we ignore the first output (_) and save the second output (predicted).\n",
    "\n",
    "##### total += labels.size(0): \n",
    "This increments the total count by the batch size (the number of images in the current batch).\n",
    "\n",
    "##### correct += (predicted == labels).sum().item(): \n",
    "This increments the correct count by the number of correct predictions in the current batch.\n",
    "\n",
    "##### print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %'): \n",
    "Finally, we print out the accuracy of our model on the test set, which is the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a76dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
